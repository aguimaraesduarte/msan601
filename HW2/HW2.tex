\documentclass[]{article}
\usepackage{lmodern}
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{listings}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{framed}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}
  \title{MSAN 601 - Homework 2}
  \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
  \author{Andre Guimaraes Duarte}
  \preauthor{\centering\large\emph}
  \postauthor{\par}
  \predate{\centering\large\emph}
  \postdate{\par}
  \date{September 12, 2016}
  
% Redefines (sub)paragraphs to behave more like section*s
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{siunitx}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\maketitle

\section*{1}
See \texttt{HW2.R} for relevant code.

In this question, we regress Length of Stay ($Y$) on Average Daily Census ($X$) from \texttt{SENIC\_data.csv}. Four linear regression models are constructed:

\begin{itemize}
\item $Y \sim X$,
\item $192Y \sim 192X$,
\item $47Y \sim X$,
\item $Y \sim 12X$.
\end{itemize}

Table \ref{q1} shows the values obtained for$b_0$, $b_1$, and $R^2$ for each model.

\begin{center}
\begin{table}[h]
\centering
\begin{tabular}{|c|S[table-format=2.5]|S[table-format=4.5]|S[table-format=3.5]|S[table-format=2.5]|}
\hline
Model & {$Y \sim X$} & {$192Y \sim 192X$} & {$47Y \sim X$} & {$Y \sim 12X$} \\
\hline
$b_0$ & 8.52093 & 1636.01874 & 400.48376 & 8.52093 \\
$b_1$ & 0.00589 & 0.00589 & 0.27688 & 0.00049 \\
$R^2$ & 0.22457 & 0.22457 & 0.22457 & 0.22457 \\
\hline
\end{tabular}
\caption{Summary of results for Question 1}
\label{q1}
\end{table}
\end{center}

We can see that the $R^2$ is the same regardless of the scalar multiplication made to $X$ and/or $Y$. When $X$ and $Y$ are multiplied by the same scalar, the slope remains the same, but the intersect changes (it is multiplied by the same scalar). If only $Y$ is multiplied by a scalar, both the slope and the intercept change in accordance to this multiplier. If only $X$ changes, the intercept remains the same while the slope varies with the inverse of the scalar.

In all cases, the coefficient of determination remains unchanged: the sample variation in $Y$ that is explained by the variation in $X$ is constant. In addition, the test statistics ($F^*$, $t^*\{b_0\}$, and $t^*\{b_1\}$) all remain constant for all four models. This can be easily proven using the definitions of $b_0$, $b_1$, $SSTO$, $SSE$, and $SSR$ (not included here, but appended at the end of the homework if relevant).

\section*{2}
\paragraph{1}
We use \texttt{platsicHardness.txt} to create a linear regression model of plastic hardness in Brinell units ($Y$) as a function of the number of hours elapsed since the plastic was molded ($X$). File \texttt{HW2.R} contains the relevant code for this question. A quick plot of the data shows a seemingly positive linear relation between $X$ and $Y$.

\paragraph{2}
The F-statistic and its associated p-value show that the linear model seems adequate for this study.

We get a coefficient of determination $R^2 = 0.9731$, meaning that $97.31$\% of the variance in plastic hardness is explained by the model. 

We get the parameter estimates for the intercept and the slope: $b_0 = 168.6$ BHN and $b_1 = 2.03438$ BHN/h. They are both significantly different from $0$, as seen from the associated p-values. This means that at time 0, the initial plastic hardness is equal to $168.6$ Brinell units. After an hour has elapsed, the mean expected increase in plastic hardness is eauql to $2.03438$ Brinell units.

In order to test the adequacy of the model, we investigate the residuals. The residual plot shows that the residuals seem homoskedastic. The sequential plot does not show any dependence between the residuals. In addition, no outliers seem to be present, as per the standardized and studentized residual plots. Finally, the residuals are normally distributed, as shown by a QQ-plot or a normality test such as Shapiro-Wilk's.

In conclusion, nothing tells us that the linear regression model is not appropriate for this study.

\paragraph{3}
We use the \texttt{leveneTest} from the package \texttt{car} with the flag \texttt{center = median} in order to perform a Brown-Forsythe test to determine whether or not the error variances varies with the level of $X$. First, however, we divide the data into two groups, $X \leq 24$ and $X>24$. Our $\alpha$ is equal to $0.05$.

We obtain a p-value of $0.2402 > \alpha$. The test is not significant, so we have no reason to reject the null hypothesis that the residual variances are homogenous. This is in accordance to our previous checks.

\section*{3}
Read.


\section*{4}
\paragraph{1}
We use \texttt{muscleMass.txt} to create a linear regression model of muscle mass ($Y$) as a function of age in women ($X$). File \texttt{HW2.R} contains the relevant code for this question. An initial plot of the data seems to show a decreasing relation between $X$ and $Y$.

The F-statistic and its associated p-value show that the linear model seems adequate for this study.

We get a coefficient of determination $R^2 = 0.7501$, meaning that $75.01$\% of the variance in plastic hardness is explained by the model. 

We get the parameter estimates for the intercept and the slope: $b_0 = 156.3466$ lbs and $b_1 = -1.19$ lbs/year. They are both significantly different from $0$, as seen from the associated p-values. The intercept in this case does not have a lot of meaning (muscle mass at age 0) and is not important for the linear model anyway. The information that this model brings is that we expect to see a mean decrease of $1.19$ lbs in muscle mass for every year in women.

In order to test the adequacy of the model, we investigate the residuals. The residual plot shows that the residuals seem homoskedastic, except for maybe one observation, number 53. The sequential plot does not show any dependence between the residuals. The standardized and studentized residual plots again show that there may be a single outlier in the data, but it is not clear. Finally, the residuals are normally distributed, as shown by a QQ-plot or a normality test such as Shapiro-Wilk's.

In conclusion, nothing tells us that the linear regression model is not appropriate for this study. There is a single point that could eventually be considered an outlier, but no immediate action is required.

\paragraph{2}
We run a Breush-Pagan test to determine whether or not the error variances vary with the level of $X$, using $\alpha = 0.01$. From the package \texttt{lmtest}, we use the function\texttt{bptest} on our linear model. We obtain a p-value of $0.0348 < \alpha$. Therefore, we reject the null hypothesis that the residual variances are homogenous. This means that the residuals fail the homoskedasticity test, and the linear model we constructed should be reevaluated before continuing.

Note: \texttt{bptest} uses studentized residuals instead of actual residuals. If we use the function \texttt{ncvTest} from the package \texttt{car}, we obtain a p-value of $0.05073 \approx \alpha$. This result if very close to $\alpha$, and it would be risky to draw a conclusion based solely on this test. This is a good example to show that it is always better to run several tests on the data instead of coming to hasty conclusions.

If we remove observation 53 (which would only be done after more serious consideration) and re-run the model, all the tests for the residuals pass (the p-value for the Breush-Pagan test is $0.1191 > \alpha$).

\section*{5}
For SLR, we know that the confidence interval (CI) and the prediction interval (PI) for a new observation $\hat{Y_h}$ are given by

CI: $\hat{Y_h} \pm t_{(1-\frac{\alpha}{2}; n-2)}s\{\hat{Y_h}\}$

PI: $\hat{Y_h} \pm t_{(1-\frac{\alpha}{2}; n-2)}s\{pred\}$

where 

$s^2\{\hat{Y_h}\} = MSE[\frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum^n_{i=1}{X_i - \bar{X}}}]$

$s^2\{pred\} = MSE + s^2\{\hat{Y_h}\} = MSE[1 + \frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum^n_{i=1}{X_i - \bar{X}}}]$

Therefore, the difference in the upper limits is given by


\begin{adjustbox}{width=1.1\linewidth,totalheight=4in, center}
\begin{tabular}{rcl}
$\hat{Y_h} + t_{(1-\frac{\alpha}{2}; n-2)}s\{pred\} - (\hat{Y_h} + t_{(1-\frac{\alpha}{2}; n-2)}s\{pred\})$ & = & $t_{(1-\frac{\alpha}{2}; n-2)}s\{pred\} - t_{(1-\frac{\alpha}{2}; n-2)}s\{\hat{Y_h}\}$ \\
& = & $t_{(1-\frac{\alpha}{2}; n-2)}[s\{pred\} - s\{\hat{Y_h}\}]$ \\
& = & $t_{(1-\frac{\alpha}{2}; n-2)} \sqrt{MSE} [\sqrt{\frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum^n_{i=1}{X_i - \bar{X}}}} - \sqrt{1 + \frac{1}{n} + \frac{(X_h - \bar{X})^2}{\sum^n_{i=1}{X_i - \bar{X}}}}]$
\end{tabular}
\end{adjustbox}

In this case, we are using \texttt{SENIC\_data.csv} and regressing Length of Stay on Infection Risk. We have

$\alpha = 0.03$,

$n = 113$,

$X_h = 11.93$,

$\hat{Y_h} = 15.40861$,

$t_{(1-\frac{\alpha}{2}; n-2)} = 2.19835$,

$MSE = 2.63752$ ($\sqrt{MSE} = 1.62404$),

$s\{pred\} = 1.29380$,

$s\{\hat{Y_h}\} = 0.293797$.

We obtain the difference between the upper limit of the confidence interval and the upper limit of the prediction interval:

$\hat{Y_h} + t_{(1-\frac{\alpha}{2}; n-2)}s\{pred\} - (\hat{Y_h} + t_{(1-\frac{\alpha}{2}; n-2)}s\{pred\}) = 2.12578$.

\section*{6}
\paragraph{1}
See \texttt{linearBC.R}.


\paragraph{2}
See \texttt{bisectionBC.R}.



\section*{Mathematical proofs for Question 1}
We have

$b_1 = \frac{\sum_{i=1}^n{(X_i - \bar{X}) \cdot (Y_i - \bar{Y})}}{\sum_{i=1}^n{(X_i - \bar{X})^2}}$ and $b_0 = \bar{Y} - b_1 \cdot \bar{X}$.

If we multiply $X$ by a constant $c_1$ and $Y$ by a constant $c_2$ ($c_1 \neq 0$ and $c_2 \neq 0$), we get

$b_{1, model} = \frac{c_1 c_2 \sum_{i=1}^n{(X_i - \bar{X}) \cdot (Y_i - \bar{Y})}}{c_1^2 \sum_{i=1}^n{(X_i - \bar{X})^2}}$ and $b_{0, model} = c_2 \bar{Y} - c_1 b_{1, model} \cdot \bar{X}$.

\begin{itemize}
\item Therefore, we can see that for the model $cY \sim cX$ ($c_1 = c_2 = c$), we get 

$b_{1, model} = \frac{\sum_{i=1}^n{(X_i - \bar{X}) \cdot (Y_i - \bar{Y})}}{\sum_{i=1}^n{(X_i - \bar{X})^2}} = b_1$ and $b_{0, model} = c (\bar{Y} - b_1 \cdot \bar{X}) = c \cdot b_0$.

In this case, the slope of the new model is unchanged, and the intercept is multiplied by the constant.

\item For the model $cY \sim X$ ($c_1 = 1$ and $c_2 = c$), we get

$b_{1, model} = \frac{c \sum_{i=1}^n{(X_i - \bar{X}) \cdot (Y_i - \bar{Y})}}{\sum_{i=1}^n{(X_i - \bar{X})^2}} = c \cdot b_1$ and $b_{0, model} = c \bar{Y} - b_{1, model} \cdot \bar{X} = c \cdot b_0$.

In this case, the slope and the intercept are both multiplied by the constant.

\item For the model $Y \sim cX$ ($c_1 = c$ and $c_2 = 1$), we get

$b_{1, model} = \frac{\sum_{i=1}^n{(X_i - \bar{X}) \cdot (Y_i - \bar{Y})}}{c \sum_{i=1}^n{(X_i - \bar{X})^2}} = \frac{1}{c} \cdot b_1$ and $b_{0, model} = \bar{Y} - c \cdot b_{1, model} \cdot \bar{X} = b_0$.

\end{itemize}

In a similar fashion, it is easy to show the following results (the sum of squares deal with $Y$, so only $c_2$ comes into play):

\begin{tabular}{lclcccc}
$SSTO_{model}$ & = & $c_2^2 \cdot SSTO$\\
$SSE_{model}$ & = & $c_2^2 \cdot SSE$ & $\Rightarrow$ & $MSE_{model}$ & = & $c_2^2 \cdot MSE$\\
$SSR_{model}$ & = & $c_2^2 \cdot SSR$ & $\Rightarrow$ & $MSR_{model}$ & = & $c_2^2 \cdot MSR$\\
\end{tabular}

Therefore, we get

\begin{itemize}
\item $R^2_{model} = \frac{SSR_{model}}{SSTO_{model}} = \frac{SSR}{SSTO} = R^2$,

\item $F^*_{model} = \frac{MSR_{model}}{MSE_{model}} = \frac{MSR}{MSE} = F^*$,

\item $s^2\{b_{1, model}\} = (\frac{c_2}{c_1})^2 s^2\{b_1\} \Rightarrow t^*\{b_{1, model}\} = \begin{cases} \frac{b_1}{\frac{c}{c} \cdot s\{b_1\}} = \frac{b_1}{s\{b_1\}}, & \mbox{if } c_1 = c_2 = c \\ \frac{c \cdot b_1}{c \cdot s\{b_1\}} = \frac{b_1}{s\{b_1\}}, & \mbox{if } c_1 = 1, c_2 = c \\ \frac{b_1 / c}{s\{b_1\} / c} = \frac{b_1}{s\{b_1\}}, & \mbox{if } c_1 = c, c_2 = 1 \end{cases}\ \  = t^*\{b_1\}$,

\item $s^2\{b_{0, model}\} = MSE_{model}[\frac{1}{n} + \frac{c_1^2 \cdot \bar{X^2}}{c_1^2 \cdot \sum_{i=1}^n{(X_i - \bar{X})^2}}] = MSE_{model}[\frac{1}{n} + \frac{\bar{X^2}}{\sum_{i=1}^n{(X_i - \bar{X})^2}}] = c_2^2 \cdot s^2\{b_0\}$.

$\Rightarrow t^*\{b_{0, model}\} = \begin{cases} \frac{c \cdot b_0}{c \cdot s\{b_0\}} = \frac{b_0}{s\{b_0\}}, & \mbox{if } c_1 = c_2 = c \\ \frac{c \cdot b_0}{c \cdot s\{b_0\}} = \frac{b_0}{s\{b_0\}}, & \mbox{if } c_1 = 1, c_2 = c \\ \frac{b_0}{s\{b_0\}}, & \mbox{if } c_1 = c, c_2 = 1 \end{cases}\ \  = t^*\{b_0\}$.

\end{itemize}

\end{document}